if((freqFrame$Var1[i] %>% as.character %>% nchar)==1){
Sum = Sum+1
}
}
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar)==1){
Sum = Sum+1
}
}
Sum =0
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar)==1){
Sum = Sum+1
}
}
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) > 1){
Sum = Sum+1
}
}
Sum =0
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) > 1){
Sum = Sum+1
}
}
Sum =0
newframe <- data.frame()
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) > 1){
Sum = Sum+1
newframe <- rbind(newframe,freqFrame[i,])
}
}
View(newframe)
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) > 1){
Sum = Sum+1
freqFrame <- freqFrame[-i,]
}
}
View(newframe)
freqFrame = as.data.frame(table(unlist(seg)))
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) > 1){
freqFrame[i,] <- NA
}
}
View(mixseg)
View(freqFrame)
freqFrame = as.data.frame(table(unlist(seg)))
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
View(freqFrame)
## 製作文字雲
library(RColorBrewer)
library(wordcloud)
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=10,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
View(freqFrame)
freqFrame$Freq[1]
freqFrame$Freq[1] >1
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
if(freqFrame$Freq[i]==1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
## 製作文字雲
library(RColorBrewer)
library(wordcloud)
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=10,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
## 利用FacebookAPI-從靠北中興收集發文資料
library(httr)
token  = "EAACEdEose0cBAJnAZCSVG9I1hEhzrtJvqB2Nv6qX6c9QQQty1PbyYivIKSZAZA96ZAZC3pkUafpMewZBYLBzIhHjm6Bn62ySpPBOWkbooLvpkDrnspHZC1vMkqN9ZCgUxPmdCazWBZA0EsfiuM0kRAIR7d8LkwbybpZCnW6IYxMTnZCBODB4GdcIrDZB0ZA8d7U0s2pUZD"
prefex = "https://graph.facebook.com/v2.12/cowbaychunghsing/?fields=posts.limit(1000)&access_token="
res    = httr::GET(url)
url    = paste0(prefex, token)
posts  = httr::content(res)
library(magrittr)
DATA = posts$posts$data %>% do.call(rbind,.) %>% data.frame
## 資料清理
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
docs <- Corpus(VectorSource(DATA$message))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs,toSpace, "#靠北中興")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
Sys.setlocale(category = "LC_ALL", locale = "cht")
mixseg = worker()
docs <- tm_map(docs, removePunctuation)
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
## 利用FacebookAPI-從靠北中興收集發文資料
library(httr)
token  = "EAACEdEose0cBAJnAZCSVG9I1hEhzrtJvqB2Nv6qX6c9QQQty1PbyYivIKSZAZA96ZAZC3pkUafpMewZBYLBzIhHjm6Bn62ySpPBOWkbooLvpkDrnspHZC1vMkqN9ZCgUxPmdCazWBZA0EsfiuM0kRAIR7d8LkwbybpZCnW6IYxMTnZCBODB4GdcIrDZB0ZA8d7U0s2pUZD"
prefex = "https://graph.facebook.com/v2.12/cowbaychunghsing/?fields=posts.limit(1000)&access_token="
url    = paste0(prefex, token)
res    = httr::GET(url)
posts  = httr::content(res)
library(magrittr)
DATA = posts$posts$data %>% do.call(rbind,.) %>% data.frame
## 資料清理
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
docs <- Corpus(VectorSource(DATA$message))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs,toSpace, "#靠北中興")
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
Sys.setlocale(category = "LC_ALL", locale = "cht")
mixseg = worker()
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單個字: Ex: 也，留下單個字以上 Ex: 不管，經過...
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
if(freqFrame$Freq[i]==1){
freqFrame[i,] <- NA
}
}
# 清除單個字: Ex: 也，留下單個字以上 Ex: 不管，經過...
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
if(freqFrame$Freq[i]==1){
freqFrame[i,] <- NA
}
}
#
Sys.Date()
## 不同的變數型態
# 整數型態 : numeric
x <- 2
## 不同的變數型態
# 整數型態 : numeric
x <- 2
x
class(x)
## 不同的變數型態
# 數值型態 : numeric
x <- 2
x
class(x)
# > [1] 2
# > [1] "numeric"
# 整數型態 : integer
x <- 2L
x
class(x)
# > [1] 2L
# > [1] "integer"
x == 1
# > [1] 2L
# > [1] "integer"
# 邏輯型態 : logical
x <- TRUE
x == 1
# 文字型態 : character
x <- "Hello world"
class(x)
# 文字型態 : character
x <- "Hello world"
class(x)
x <- 'Hello world'
class(x)
# 日期型態 : Date
x <- 	Sys.Date()
class(x)
x
# >[1] "2018-03-29"
# >[1] "Date"
# 時間型態 : 	POSIXct POSIXt
x <- Sys.time()
class(x)
x
# 你也可以使用assign function
assign(x,6)
# 你也可以使用assign function
assign(x,6)
assign(6,x)
?assign
# 你也可以使用assign function
assign(X,87)
X = 5
# 你也可以使用assign function
assign(X,87)
# 你也可以使用assign function
assign(X,87)
# 你也可以使用assign function
assign(Data,87)
# 你也可以使用assign function
assign("X",87)
# 你也可以使用assign function
assign("X",6)
## 賦值Assign
# <- or =
"X" <- 5
# 你也可以使用assign function
assign("X",6)
X
knitr::opts_chunk$set(echo = TRUE)
# 數值型態 : numeric
x <- 2
x
class(x)
# 數值型態 : numeric
x <- 2
x
class(x)
# 數值型態 : numeric
x <- 2
x
# 可以使用class判別變數是哪種變數型態
class(x)
# 整數型態 : integer
x <- 2L
x
class(x)
# 邏輯型態 : logical
x <- TRUE
y <- FALSE
x == 1
y == 0
# 文字型態 : character
x <- "Hello world"
x <- 'Hello world'
class(x)
# 日期型態 : Date
x <- Sys.Date()
class(x)
x
# 時間型態 : 	POSIXct POSIXt
x <- Sys.time()
class(x)
x
token  = "EAACEdEose0cBAKi2Fwc2bFZCLTZCvYebGLUU13vtB1XGf8U3LeTkqXRjnTTD1ZBprZCZAaU240rLLdrEKvKkVpwuMIJEOZBLmzu4EcGNqVA3tlBt9iXlk37QHDi7GekHpwzCtJdVnu59K9VrOkTdSMQpD7tu7EguCn6xgFOMoq3eY5uBhBd2bSXboLRxXXrJQZD"
## 利用FacebookAPI-從靠收集發文資料
library(httr)
token  = "EAACEdEose0cBAKi2Fwc2bFZCLTZCvYebGLUU13vtB1XGf8U3LeTkqXRjnTTD1ZBprZCZAaU240rLLdrEKvKkVpwuMIJEOZBLmzu4EcGNqVA3tlBt9iXlk37QHDi7GekHpwzCtJdVnu59K9VrOkTdSMQpD7tu7EguCn6xgFOMoq3eY5uBhBd2bSXboLRxXXrJQZD"
prefex = "https://graph.facebook.com/v2.12/Biotechissuck/?fields=posts.limit(100)&access_token="
url    = paste0(prefex, token)
res    = httr::GET(url)
posts  = httr::content(res)
library(magrittr)
DATA = posts$posts$data %>% do.call(rbind,.) %>% data.frame
View(DATA)
DATA[1,79]
DATA[1:79]
DATA[,1]
DATA[1,]
DATA[1:80,]
DATA <- DATA[1:80,]
View(DATA)
DATA$message
message <- DATA$message
View(message)
View(message)
message <- DATA$message
View(message)
message <- DATA[,-3]
View(message)
message <- message[,-3]
View(message)
write.csv(x = message,file = 'output.csv')
View(message)
write.csv(message,file = "test.csv")
write.csv(message,file = "test.csv")
View(message)
message <- message[,-1]
View(message)
DATA <- DATA[1:80,]
View(DATA)
write.csv(DATA,file = "test.csv")
message
View(message)
write.csv2(DATA,file="text.csv")
df <- apply(DATA,2,as.character)
View(df)
write.csv2(DATA,file="text.csv")
write.csv2(df,file="text.csv")
getwd()
getwd()
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
library(NLP)
library(tm)
install.packages("tm")
installed.packages("slam")
library(NLP)
library(tm)
install.packages("slam")
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data cleaning : remove Punctuation, Numbers, Whitespace anf Eng.
# I found that using RemovePunctuation will
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, toSpace, "、")
docs <- tm_map(docs, toSpace, "，")
docs <- tm_map(docs, toSpace, "。")
docs <- tm_map(docs, toSpace, "！")
docs <- tm_map(docs, toSpace, "「")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "」")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
inspect(docs)
# words cut
mixseg = worker()
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
d.corpus <- Corpus(VectorSource(seg))
Sys.setlocale(locale="English")
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
# tf-idf computation
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
# Data Visualization
# Word TF-IDF frequencics
freq=rowSums(as.matrix(tfidfnn))
tail(freq,10)
plot(sort(freq, decreasing = T),col="blue",main="Word TF-IDF frequencies", xlab="TF-IDF-based rank", ylab = "TF-IDF")
#
library(ggplot2)
high.freq=tail(sort(freq),n=10)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df)
tail(sort(freq),n=10)
ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Terms") + ylab("Frequency") +
ggtitle("Term frequencies")
write.csv(tfidfnn, "show.csv")
tdm <- TermDocumentMatrix(d.corpus)
docs <- tm_map(docs, stripWhitespace)
# Clean the file: 做TDM時會產稱<a06>之類的亂碼
for(i in 1:length(DF$.rownames)){
if(grepl("<",DF$.rownames[i])==1){
DF[i,] = NA
}
}
DF <- na.omit(DF)
setwd("~/GitHub/NTU_code/NTU_code/week_5/hw_5")
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
# Open the text file of President’s New Year’s Day Message
# (from The 86th year of the Republic Era to 105th)
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data cleaning : remove Punctuation, Numbers, Whitespace anf Eng.
# I found that using RemovePunctuation will
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, toSpace, "、")
docs <- tm_map(docs, toSpace, "，")
docs <- tm_map(docs, toSpace, "。")
docs <- tm_map(docs, toSpace, "！")
docs <- tm_map(docs, toSpace, "「")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "」")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
# words cut
mixseg = worker()
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
d.corpus <- Corpus(VectorSource(seg))
Sys.setlocale(locale="English")
tdm <- TermDocumentMatrix(d.corpus)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
# Clean the file: 做TDM時會產稱<a06>之類的亂碼
for(i in 1:length(DF$.rownames)){
if(grepl("<",DF$.rownames[i])==1){
DF[i,] = NA
}
}
DF <- na.omit(DF)
# tf-idf computation
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]
# Data Visualization
# Word TF-IDF frequencics
freq=rowSums(as.matrix(tfidfnn))
tail(freq,10)
plot(sort(freq, decreasing = T),col="blue",main="Word TF-IDF frequencies", xlab="TF-IDF-based rank", ylab = "TF-IDF")
#
library(ggplot2)
high.freq=tail(sort(freq),n=10)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df)
tail(sort(freq),n=10)
ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
geom_bar(stat="identity") + coord_flip() +
xlab("Terms") + ylab("Frequency") +
ggtitle("Term frequencies")
write.csv(tfidfnn, "show.csv")
Sys.getlocale(category = "LC_ALL")
View(DF)
